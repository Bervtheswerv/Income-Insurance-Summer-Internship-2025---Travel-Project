{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e410c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ACCIDENT DESCRIPTION NLP CLASSIFIER - JUPYTER NOTEBOOK STYLE\n",
    "# Advanced classification using spaCy, scikit-learn, and semantic analysis\n",
    "# ============================================================================\n",
    "\n",
    "# BLOCK 1: INSTALL AND IMPORT PACKAGES\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "First, install required packages:\n",
    "pip install pandas numpy scikit-learn spacy nltk wordcloud matplotlib seaborn\n",
    "python -m spacy download en_core_web_sm\n",
    "pip install sentence-transformers textblob\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP packages\n",
    "import spacy\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "print(\"‚úÖ All packages imported successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 2: LOAD AND EXPLORE DATA\n",
    "# ============================================================================\n",
    "\n",
    "# Load the data\n",
    "input_file = '/Users/bervynwong/Desktop/INCOME Travel Insurance Portfolio Analysis Project/Profitability Model/Free Text classification/accident_description_raw.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "print(f\"üìä Dataset loaded: {len(df):,} records\")\n",
    "print(f\"üìã Columns: {list(df.columns)}\")\n",
    "print(f\"üìÑ Sample descriptions:\")\n",
    "\n",
    "# Display sample descriptions\n",
    "for i in range(5):\n",
    "    print(f\"\\n{i+1}. {df['AccidentDesc'].iloc[i]}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nüîç Missing values in AccidentDesc: {df['AccidentDesc'].isnull().sum()}\")\n",
    "print(f\"üîç Empty descriptions: {(df['AccidentDesc'] == '').sum()}\")\n",
    "\n",
    "# Basic statistics\n",
    "df['desc_length'] = df['AccidentDesc'].str.len()\n",
    "print(f\"\\nüìà Description length stats:\")\n",
    "print(df['desc_length'].describe())\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 3: ADVANCED TEXT PREPROCESSING WITH SPACY\n",
    "# ============================================================================\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def advanced_text_preprocessing(text):\n",
    "    \"\"\"Advanced text preprocessing using spaCy\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Process with spaCy\n",
    "    doc = nlp(text.lower())\n",
    "    \n",
    "    # Extract meaningful tokens (remove stop words, punctuation, spaces)\n",
    "    tokens = []\n",
    "    medical_entities = []\n",
    "    \n",
    "    for token in doc:\n",
    "        # Skip stop words, punctuation, and spaces\n",
    "        if not token.is_stop and not token.is_punct and not token.is_space and len(token.text) > 2:\n",
    "            # Use lemmatized form\n",
    "            tokens.append(token.lemma_)\n",
    "        \n",
    "        # Extract medical/injury related entities\n",
    "        if token.pos_ in ['NOUN', 'ADJ'] and any(keyword in token.text.lower() for keyword in \n",
    "            ['pain', 'injury', 'sick', 'ill', 'hurt', 'ache', 'fever', 'stomach', 'head', 'knee', 'back']):\n",
    "            medical_entities.append(token.text.lower())\n",
    "    \n",
    "    # Extract named entities (medical conditions, body parts)\n",
    "    entities = [(ent.text.lower(), ent.label_) for ent in doc.ents \n",
    "                if ent.label_ in ['PERSON', 'ORG', 'GPE', 'PRODUCT']]\n",
    "    \n",
    "    return {\n",
    "        'cleaned_text': ' '.join(tokens),\n",
    "        'medical_entities': medical_entities,\n",
    "        'named_entities': entities,\n",
    "        'token_count': len(tokens)\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"üîÑ Preprocessing text with spaCy...\")\n",
    "preprocessing_results = df['AccidentDesc'].apply(advanced_text_preprocessing)\n",
    "\n",
    "# Extract results\n",
    "df['cleaned_text'] = [result['cleaned_text'] for result in preprocessing_results]\n",
    "df['medical_entities'] = [result['medical_entities'] for result in preprocessing_results]\n",
    "df['token_count'] = [result['token_count'] for result in preprocessing_results]\n",
    "\n",
    "print(\"‚úÖ Text preprocessing completed!\")\n",
    "print(f\"üìä Average tokens per description: {df['token_count'].mean():.1f}\")\n",
    "\n",
    "# Show preprocessing examples\n",
    "print(f\"\\nüìù Preprocessing examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df['AccidentDesc'].iloc[i]}\")\n",
    "    print(f\"Cleaned:  {df['cleaned_text'].iloc[i]}\")\n",
    "    print(f\"Medical entities: {df['medical_entities'].iloc[i]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 4: TF-IDF ANALYSIS AND KEYWORD DISCOVERY\n",
    "# ============================================================================\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    min_df=5,  # Ignore terms that appear in less than 5 documents\n",
    "    max_df=0.7,  # Ignore terms that appear in more than 70% of documents\n",
    "    ngram_range=(1, 2),  # Include unigrams and bigrams\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Fit TF-IDF on cleaned text\n",
    "print(\"üîÑ Computing TF-IDF vectors...\")\n",
    "tfidf_matrix = tfidf.fit_transform(df['cleaned_text'])\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "print(f\"‚úÖ TF-IDF completed! Matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Get top keywords overall\n",
    "def get_top_keywords(tfidf_matrix, feature_names, top_n=20):\n",
    "    \"\"\"Get top keywords by TF-IDF score\"\"\"\n",
    "    # Sum TF-IDF scores across all documents\n",
    "    scores = np.array(tfidf_matrix.sum(axis=0)).flatten()\n",
    "    \n",
    "    # Create keyword-score pairs and sort\n",
    "    keyword_scores = list(zip(feature_names, scores))\n",
    "    keyword_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return keyword_scores[:top_n]\n",
    "\n",
    "top_keywords = get_top_keywords(tfidf_matrix, feature_names, 30)\n",
    "\n",
    "print(f\"\\nüîù Top 30 keywords by TF-IDF importance:\")\n",
    "for i, (keyword, score) in enumerate(top_keywords, 1):\n",
    "    print(f\"{i:2d}. {keyword:<20} (score: {score:.3f})\")\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 5: SEMANTIC CLUSTERING FOR AUTOMATIC CATEGORY DISCOVERY\n",
    "# ============================================================================\n",
    "\n",
    "# Use K-means clustering to discover natural groupings\n",
    "print(\"\\nüîÑ Discovering natural categories using K-means clustering...\")\n",
    "\n",
    "# Try different numbers of clusters\n",
    "cluster_range = range(3, 8)\n",
    "inertias = []\n",
    "\n",
    "for n_clusters in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    kmeans.fit(tfidf_matrix)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cluster_range, inertias, 'bo-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Use optimal number of clusters (let's start with 5 based on user's categories)\n",
    "optimal_clusters = 5\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "df['auto_cluster'] = cluster_labels\n",
    "\n",
    "print(f\"‚úÖ Clustering completed with {optimal_clusters} clusters\")\n",
    "print(f\"üìä Cluster distribution:\")\n",
    "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"Cluster {cluster_id}: {count:,} records ({percentage:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 6: ANALYZE CLUSTERS AND EXTRACT THEMES\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_cluster_themes(cluster_id, top_n=15):\n",
    "    \"\"\"Analyze what each cluster represents\"\"\"\n",
    "    cluster_mask = df['auto_cluster'] == cluster_id\n",
    "    cluster_docs = df[cluster_mask]['cleaned_text'].values\n",
    "    \n",
    "    if len(cluster_docs) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Get TF-IDF for this cluster\n",
    "    cluster_tfidf = tfidf.transform(cluster_docs)\n",
    "    \n",
    "    # Calculate mean TF-IDF scores for this cluster\n",
    "    mean_scores = np.array(cluster_tfidf.mean(axis=0)).flatten()\n",
    "    \n",
    "    # Get top keywords for this cluster\n",
    "    keyword_scores = list(zip(feature_names, mean_scores))\n",
    "    keyword_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return keyword_scores[:top_n]\n",
    "\n",
    "print(\"\\nüéØ CLUSTER THEMES ANALYSIS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cluster_themes = {}\n",
    "for cluster_id in range(optimal_clusters):\n",
    "    themes = analyze_cluster_themes(cluster_id)\n",
    "    cluster_themes[cluster_id] = themes\n",
    "    \n",
    "    print(f\"\\nüìÇ CLUSTER {cluster_id} (n={cluster_counts[cluster_id]:,}):\")\n",
    "    print(\"Top keywords:\", \", \".join([kw for kw, score in themes[:10]]))\n",
    "    \n",
    "    # Show sample descriptions\n",
    "    cluster_samples = df[df['auto_cluster'] == cluster_id]['AccidentDesc'].head(3)\n",
    "    print(\"Sample descriptions:\")\n",
    "    for i, desc in enumerate(cluster_samples, 1):\n",
    "        print(f\"  {i}. {desc[:100]}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 7: SMART CATEGORY MAPPING USING MEDICAL NER AND SEMANTIC ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def classify_using_semantic_analysis(text, medical_entities):\n",
    "    \"\"\"Classify using semantic analysis and medical entities\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Define semantic keyword groups (more comprehensive than manual keywords)\n",
    "    illness_indicators = {\n",
    "        'symptoms': ['fever', 'cough', 'headache', 'pain', 'ache', 'sick', 'ill', 'flu', 'cold'],\n",
    "        'conditions': ['allergic', 'reaction', 'infection', 'virus', 'bacterial', 'respiratory'],\n",
    "        'treatments': ['medication', 'treatment', 'hospital', 'emergency', 'medical']\n",
    "    }\n",
    "    \n",
    "    accident_indicators = {\n",
    "        'injuries': ['fracture', 'broken', 'sprain', 'bruise', 'cut', 'wound', 'injury', 'hurt'],\n",
    "        'body_parts': ['knee', 'shoulder', 'back', 'neck', 'ankle', 'arm', 'leg', 'head', 'ribs'],\n",
    "        'incidents': ['accident', 'collision', 'fall', 'fell', 'slip', 'crash', 'hit', 'impact']\n",
    "    }\n",
    "    \n",
    "    food_indicators = {\n",
    "        'poisoning': ['food poisoning', 'stomach', 'nausea', 'vomit', 'diarrhea', 'diarrhoea'],\n",
    "        'digestive': ['gastro', 'intestinal', 'digestive', 'bowel', 'abdominal'],\n",
    "        'food_types': ['shellfish', 'seafood', 'restaurant', 'cafe', 'meal']\n",
    "    }\n",
    "    \n",
    "    # Calculate semantic scores\n",
    "    illness_score = sum([\n",
    "        sum(1 for keyword in keywords if keyword in text_lower)\n",
    "        for keywords in illness_indicators.values()\n",
    "    ])\n",
    "    \n",
    "    accident_score = sum([\n",
    "        sum(1 for keyword in keywords if keyword in text_lower)\n",
    "        for keywords in accident_indicators.values()\n",
    "    ])\n",
    "    \n",
    "    food_score = sum([\n",
    "        sum(1 for keyword in keywords if keyword in text_lower)\n",
    "        for keywords in food_indicators.values()\n",
    "    ])\n",
    "    \n",
    "    # Boost scores based on medical entities\n",
    "    medical_boost = len(medical_entities) * 0.5\n",
    "    if any(entity in ['stomach', 'digestive', 'intestinal'] for entity in medical_entities):\n",
    "        food_score += medical_boost\n",
    "    elif any(entity in ['fever', 'headache', 'respiratory'] for entity in medical_entities):\n",
    "        illness_score += medical_boost\n",
    "    elif any(entity in ['injury', 'fracture', 'sprain'] for entity in medical_entities):\n",
    "        accident_score += medical_boost\n",
    "    \n",
    "    return {\n",
    "        'general_illness': illness_score > 0,\n",
    "        'travel_accident': accident_score > 0,\n",
    "        'food_poisoning': food_score > 0,\n",
    "        'scores': {\n",
    "            'illness_score': illness_score,\n",
    "            'accident_score': accident_score,\n",
    "            'food_score': food_score\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"üîÑ Applying semantic classification...\")\n",
    "\n",
    "# Apply semantic classification\n",
    "semantic_results = []\n",
    "for idx, row in df.iterrows():\n",
    "    result = classify_using_semantic_analysis(row['cleaned_text'], row['medical_entities'])\n",
    "    semantic_results.append(result)\n",
    "\n",
    "# Extract classification results\n",
    "df['general_illness_flag'] = [result['general_illness'] for result in semantic_results]\n",
    "df['travel_accident_flag'] = [result['travel_accident'] for result in semantic_results]\n",
    "df['food_poisoning_flag'] = [result['food_poisoning'] for result in semantic_results]\n",
    "\n",
    "# Calculate category statistics\n",
    "category_columns = ['general_illness_flag', 'travel_accident_flag', 'food_poisoning_flag']\n",
    "df['category_count'] = df[category_columns].sum(axis=1)\n",
    "\n",
    "# Create category labels\n",
    "def get_category_labels(row):\n",
    "    categories = []\n",
    "    if row['general_illness_flag']:\n",
    "        categories.append('General Illness')\n",
    "    if row['travel_accident_flag']:\n",
    "        categories.append('Travel/Accident')\n",
    "    if row['food_poisoning_flag']:\n",
    "        categories.append('Food Poisoning')\n",
    "    \n",
    "    return ', '.join(categories) if categories else 'Uncategorized'\n",
    "\n",
    "df['semantic_categories'] = df.apply(get_category_labels, axis=1)\n",
    "\n",
    "print(\"‚úÖ Semantic classification completed!\")\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 8: RESULTS ANALYSIS AND VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "# Generate comprehensive summary\n",
    "print(\"\\nüìä SEMANTIC CLASSIFICATION RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "total_records = len(df)\n",
    "general_illness_count = df['general_illness_flag'].sum()\n",
    "travel_accident_count = df['travel_accident_flag'].sum()\n",
    "food_poisoning_count = df['food_poisoning_flag'].sum()\n",
    "multiple_categories = len(df[df['category_count'] > 1])\n",
    "uncategorized = len(df[df['category_count'] == 0])\n",
    "classification_rate = ((total_records - uncategorized) / total_records) * 100\n",
    "\n",
    "print(f\"Total Records: {total_records:,}\")\n",
    "print(f\"Classification Rate: {classification_rate:.1f}%\")\n",
    "print(f\"Uncategorized: {uncategorized:,} ({(uncategorized/total_records)*100:.1f}%)\")\n",
    "print(f\"Multiple Categories: {multiple_categories:,} ({(multiple_categories/total_records)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nCATEGORY BREAKDOWN:\")\n",
    "print(f\"General Illness: {general_illness_count:,} ({(general_illness_count/total_records)*100:.1f}%)\")\n",
    "print(f\"Travel/Accident: {travel_accident_count:,} ({(travel_accident_count/total_records)*100:.1f}%)\")\n",
    "print(f\"Food Poisoning: {food_poisoning_count:,} ({(food_poisoning_count/total_records)*100:.1f}%)\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Category distribution\n",
    "category_counts = [general_illness_count, travel_accident_count, food_poisoning_count, uncategorized]\n",
    "category_labels = ['General Illness', 'Travel/Accident', 'Food Poisoning', 'Uncategorized']\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
    "\n",
    "axes[0, 0].pie(category_counts, labels=category_labels, autopct='%1.1f%%', colors=colors)\n",
    "axes[0, 0].set_title('Category Distribution')\n",
    "\n",
    "# Cluster vs Semantic Category comparison\n",
    "cluster_category_crosstab = pd.crosstab(df['auto_cluster'], df['semantic_categories'])\n",
    "sns.heatmap(cluster_category_crosstab, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Auto Clusters vs Semantic Categories')\n",
    "axes[0, 1].set_xlabel('Semantic Categories')\n",
    "axes[0, 1].set_ylabel('Auto Clusters')\n",
    "\n",
    "# Description length by category\n",
    "category_lengths = df.groupby('semantic_categories')['desc_length'].mean()\n",
    "axes[1, 0].bar(range(len(category_lengths)), category_lengths.values)\n",
    "axes[1, 0].set_xticks(range(len(category_lengths)))\n",
    "axes[1, 0].set_xticklabels(category_lengths.index, rotation=45)\n",
    "axes[1, 0].set_title('Average Description Length by Category')\n",
    "axes[1, 0].set_ylabel('Characters')\n",
    "\n",
    "# Multiple categories analysis\n",
    "multi_cat_df = df[df['category_count'] > 1]\n",
    "if len(multi_cat_df) > 0:\n",
    "    multi_cat_counts = multi_cat_df['semantic_categories'].value_counts()\n",
    "    axes[1, 1].bar(range(len(multi_cat_counts)), multi_cat_counts.values)\n",
    "    axes[1, 1].set_xticks(range(len(multi_cat_counts)))\n",
    "    axes[1, 1].set_xticklabels(multi_cat_counts.index, rotation=45)\n",
    "    axes[1, 1].set_title('Multiple Category Combinations')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No multiple\\ncategories found', \n",
    "                    ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].set_title('Multiple Category Combinations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 9: SAVE RESULTS AND CREATE CATEGORY-SPECIFIC DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "# Save main classified dataset\n",
    "output_file = '/Users/bervynwong/Desktop/INCOME Travel Insurance Portfolio Analysis Project/Profitability Model/Free Text classification/accident_description_classified.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"üíæ Main classified dataset saved to: {output_file}\")\n",
    "\n",
    "# Create category-specific datasets\n",
    "categories = {\n",
    "    'general_illness': df[df['general_illness_flag'] == True],\n",
    "    'travel_accident': df[df['travel_accident_flag'] == True],\n",
    "    'food_poisoning': df[df['food_poisoning_flag'] == True],\n",
    "    'multiple_categories': df[df['category_count'] > 1],\n",
    "    'uncategorized': df[df['category_count'] == 0]\n",
    "}\n",
    "\n",
    "print(f\"\\nüíæ Creating category-specific datasets:\")\n",
    "for category_name, category_df in categories.items():\n",
    "    if len(category_df) > 0:\n",
    "        filename = f\"{category_name}_descriptions_nlp.csv\"\n",
    "        category_df.to_csv(filename, index=False)\n",
    "        print(f\"‚úÖ {category_name.replace('_', ' ').title()}: {len(category_df):,} records ‚Üí {filename}\")\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 10: SAMPLE CLASSIFICATIONS AND VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüîç SAMPLE CLASSIFICATIONS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show examples from each category\n",
    "for category in ['General Illness', 'Travel/Accident', 'Food Poisoning']:\n",
    "    category_samples = df[df['semantic_categories'] == category].head(3)\n",
    "    if len(category_samples) > 0:\n",
    "        print(f\"\\nüìÇ {category.upper()} SAMPLES:\")\n",
    "        for idx, row in category_samples.iterrows():\n",
    "            print(f\"Description: {row['AccidentDesc']}\")\n",
    "            print(f\"Cleaned: {row['cleaned_text'][:100]}...\")\n",
    "            print(f\"Medical entities: {row['medical_entities']}\")\n",
    "            print(f\"Auto cluster: {row['auto_cluster']}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "# Show multiple category examples\n",
    "multi_samples = df[df['category_count'] > 1].head(3)\n",
    "if len(multi_samples) > 0:\n",
    "    print(f\"\\nüìÇ MULTIPLE CATEGORIES SAMPLES:\")\n",
    "    for idx, row in multi_samples.iterrows():\n",
    "        print(f\"Description: {row['AccidentDesc']}\")\n",
    "        print(f\"Categories: {row['semantic_categories']}\")\n",
    "        print(f\"Flags: Illness={row['general_illness_flag']}, Accident={row['travel_accident_flag']}, Food={row['food_poisoning_flag']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "print(f\"\\nüéâ NLP Classification completed successfully!\")\n",
    "print(f\"üìà Total processing: {len(df):,} records classified with {classification_rate:.1f}% success rate\")\n",
    "print(f\"üìÅ Output files created: {output_file} + category-specific CSV files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
